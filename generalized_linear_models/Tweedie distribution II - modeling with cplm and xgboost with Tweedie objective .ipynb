{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive modeling with Tweedie distributed response  \n",
    "*Gorkem Ozkaya*\n",
    "\n",
    "In this notebook we generate data according to the linear Tweedie model with a log link. Then we compare three predictive models: \n",
    "* Tweedie GLM with the `cplm` package\n",
    "* Gradient boosting with the `xgboost` package, using the `reg:tweedie` objective, where the loss is likelihood of the mean for Tweedie distribution\n",
    "* Gradient boosting with the `xgboost` package, using the `reg:linear` objective, where the loss is simple least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      ": package ‘cplm’ was built under R version 3.3.2Loading required package: coda\n",
      "Warning message:\n",
      ": package ‘coda’ was built under R version 3.3.2Loading required package: Matrix\n",
      "Loading required package: splines\n"
     ]
    }
   ],
   "source": [
    "library(tweedie)\n",
    "library(cplm)\n",
    "library(xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that generates data according to the linear model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_data <- function (N = 10000, p = 1.5, phi=100) {\n",
    "  var1 <- rnorm(N)\n",
    "  var2 <- rnorm(N)\n",
    "  var3 <- rnorm(N)\n",
    "  y <- exp(7 + 0.1*var1 + 0.2*var2 - 0.3*var3)\n",
    "  \n",
    "  resp = rep(0, N)\n",
    "  \n",
    "  for (i in 1:N) {\n",
    "    resp[i] <- rtweedie(1, xi = p, mu = y[i], phi=phi )\n",
    "  }\n",
    "  \n",
    "  return(data.frame(var1, var2, var3, mu=y, resp))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt <- gen_data()\n",
    "dt_test <- gen_data()\n",
    "\n",
    "# for xgboost models\n",
    "d_train <- xgb.DMatrix(data = as.matrix(dt[, c(\"var1\", \"var2\", \"var3\")]), label = dt$resp, missing = NA)\n",
    "d_test <- xgb.DMatrix(data = as.matrix(dt_test[, c(\"var1\", \"var2\", \"var3\")]), label = dt_test$resp, missing = NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweedie GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_glm <- cpglm(resp ~ var1+var2+var3, data=dt, optimizer=\"bobyqa\")\n",
    "\n",
    "y_hat_glm <- predict(model_glm, dt_test[, c(\"var1\", \"var2\", \"var3\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost with Tweedie objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:2476.955811\ttest-rmse:2351.621582 \n",
      "[51]\ttrain-rmse:2475.586670\ttest-rmse:2350.224854 \n",
      "[101]\ttrain-rmse:2471.941650\ttest-rmse:2346.528320 \n",
      "[151]\ttrain-rmse:2462.552734\ttest-rmse:2337.097168 \n",
      "[201]\ttrain-rmse:2440.162842\ttest-rmse:2314.962891 \n",
      "[251]\ttrain-rmse:2394.323730\ttest-rmse:2271.380859 \n",
      "[301]\ttrain-rmse:2320.444580\ttest-rmse:2206.259766 \n",
      "[351]\ttrain-rmse:2230.586426\ttest-rmse:2137.055908 \n",
      "[401]\ttrain-rmse:2145.951416\ttest-rmse:2083.771240 \n",
      "[451]\ttrain-rmse:2079.055176\ttest-rmse:2053.506104 \n",
      "[501]\ttrain-rmse:2030.420288\ttest-rmse:2039.423462 \n",
      "[551]\ttrain-rmse:1997.380737\ttest-rmse:2033.699219 \n",
      "[601]\ttrain-rmse:1974.729614\ttest-rmse:2031.606323 \n",
      "[651]\ttrain-rmse:1958.262329\ttest-rmse:2031.567017 \n",
      "[701]\ttrain-rmse:1945.983887\ttest-rmse:2031.985962 \n",
      "[751]\ttrain-rmse:1935.498047\ttest-rmse:2032.644165 \n",
      "[801]\ttrain-rmse:1926.630981\ttest-rmse:2033.466919 \n",
      "[851]\ttrain-rmse:1918.441528\ttest-rmse:2034.266846 \n",
      "[901]\ttrain-rmse:1910.293213\ttest-rmse:2035.266479 \n",
      "[951]\ttrain-rmse:1901.974609\ttest-rmse:2036.184326 \n",
      "[1000]\ttrain-rmse:1893.611450\ttest-rmse:2036.960571 \n"
     ]
    }
   ],
   "source": [
    "params_tweedie <- list(\n",
    "  objective = 'reg:tweedie',\n",
    "  eval_metric = 'rmse', \n",
    "  tweedie_variance_power = 1.5,\n",
    "  max_depth = 6,\n",
    "  eta = 0.01)\n",
    "\n",
    "\n",
    "bst_tweedie <- xgb.train(\n",
    "  data = d_train, \n",
    "  params = params_tweedie, \n",
    "  maximize = FALSE,\n",
    "  watchlist = list(train = d_train, test=d_test), \n",
    "  nrounds = 1000,\n",
    "  print_every_n = 50)\n",
    "\n",
    "\n",
    "y_hat_xgb_tweedie <- predict(bst_tweedie, d_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost with Least squares objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:2469.842773\ttest-rmse:2345.668457 \n",
      "[51]\ttrain-rmse:2204.150879\ttest-rmse:2137.756592 \n",
      "[101]\ttrain-rmse:2072.081299\ttest-rmse:2061.310059 \n",
      "[151]\ttrain-rmse:2003.094116\ttest-rmse:2037.714478 \n",
      "[201]\ttrain-rmse:1962.820557\ttest-rmse:2031.466309 \n",
      "[251]\ttrain-rmse:1935.417114\ttest-rmse:2031.223633 \n",
      "[301]\ttrain-rmse:1917.244751\ttest-rmse:2032.850098 \n",
      "[351]\ttrain-rmse:1901.709961\ttest-rmse:2035.406616 \n",
      "[401]\ttrain-rmse:1887.165039\ttest-rmse:2037.472778 \n",
      "[451]\ttrain-rmse:1877.370850\ttest-rmse:2039.023193 \n",
      "[501]\ttrain-rmse:1869.997681\ttest-rmse:2040.524902 \n",
      "[551]\ttrain-rmse:1861.443359\ttest-rmse:2042.084961 \n",
      "[601]\ttrain-rmse:1853.184937\ttest-rmse:2043.426636 \n",
      "[651]\ttrain-rmse:1843.502197\ttest-rmse:2045.074707 \n",
      "[701]\ttrain-rmse:1834.347168\ttest-rmse:2046.445557 \n",
      "[751]\ttrain-rmse:1824.339111\ttest-rmse:2047.975708 \n",
      "[801]\ttrain-rmse:1814.281372\ttest-rmse:2049.369385 \n",
      "[851]\ttrain-rmse:1803.211304\ttest-rmse:2050.776367 \n",
      "[901]\ttrain-rmse:1793.056763\ttest-rmse:2052.099609 \n",
      "[951]\ttrain-rmse:1783.048584\ttest-rmse:2053.747559 \n",
      "[1000]\ttrain-rmse:1772.775757\ttest-rmse:2055.662109 \n"
     ]
    }
   ],
   "source": [
    "params_leastsq <- list(\n",
    "  objective = 'reg:linear',\n",
    "  eval_metric = 'rmse', \n",
    "  tweedie_variance_power = 1.5,\n",
    "  max_depth = 6,\n",
    "  eta = 0.01)\n",
    "\n",
    "\n",
    "bst_leastsq <- xgb.train(\n",
    "  data = d_train, \n",
    "  params = params_leastsq, \n",
    "  maximize = FALSE,\n",
    "  watchlist = list(train = d_train, test=d_test), \n",
    "  nrounds = 1000,\n",
    "  print_every_n = 50)\n",
    "\n",
    "\n",
    "y_hat_xgb_leastsq <- predict(bst_leastsq, d_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the results of the three models\n",
    "Now we compare the model performances on the test sets using Gini indices, which is one of the standard performance measures in insurance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "gini(loss = \"y\", score = c(\"glm\", \"xgb_tweedie\", \"xgb_leastsq\"), \n",
       "    base = NULL, data = df_gini)\n",
       "\n",
       "Gini indices:\n",
       "             glm     xgb_tweedie  xgb_leastsq\n",
       "glm           0.000   2.608        2.151     \n",
       "xgb_tweedie  13.442   0.000        7.222     \n",
       "xgb_leastsq  12.562   6.847        0.000     \n",
       "\n",
       "Standard errors:\n",
       "             glm     xgb_tweedie  xgb_leastsq\n",
       "glm          0.0000  1.0031       0.9902     \n",
       "xgb_tweedie  1.0469  0.0000       1.0563     \n",
       "xgb_leastsq  0.9947  1.0205       0.0000     \n",
       "\n",
       "The selected score is glm."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_gini <- data.frame(y = dt_test$resp, glm = y_hat_glm, xgb_tweedie = y_hat_xgb_tweedie, xgb_leastsq = y_hat_xgb_leastsq)\n",
    "df_gini$base = mean(df_gini$y)\n",
    "gini(loss = \"y\" , score = c(\"glm\", \"xgb_tweedie\", \"xgb_leastsq\"), base = NULL, data=df_gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Discussion\n",
    "Looking at the Gini indices, we see that GLM performs the best.  This is expected, because we generated the data completely in accordance with the GLM Tweedie model assumptions.  \n",
    "\n",
    "On the other hand, when we compare the XGBoost performances, we see no significant difference between using  *least squares* versus *Tweedie* objectives. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
